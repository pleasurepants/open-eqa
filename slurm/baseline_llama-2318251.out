### Starting TaskPrologue of job 2318251 on a0702 at Tue Jan 21 10:34:58 CET 2025
Running on cores 0-63 with governor ondemand
Tue Jan 21 10:34:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:0E:00.0 Off |                    0 |
| N/A   35C    P0             54W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:13:00.0 Off |                    0 |
| N/A   33C    P0             54W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:49:00.0 Off |                    0 |
| N/A   33C    P0             52W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:4F:00.0 Off |                    0 |
| N/A   35C    P0             54W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

Tue Jan 21 10:35:00 CET 2025
a0702.nhr.fau.de
/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/bin/python
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
found 1,636 questions
found 1,636 questions
found 1,636 questions
found 1,636 questions
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/15 [00:10<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/15 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 160, in <module>
Traceback (most recent call last):
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 160, in <module>
    main(parse_args())
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 118, in main
    main(parse_args())
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 118, in main
    model = LLaMARunner(
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/utils/llama_utils.py", line 29, in __init__
    model = LLaMARunner(
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/utils/llama_utils.py", line 29, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    return model_class.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    ) = cls._load_pretrained_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 812, in _load_state_dict_into_meta_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 812, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 594.69 MiB is free. Including non-PyTorch memory, this process has 11.89 GiB memory in use. Process 3826478 has 11.89 GiB memory in use. Process 3826479 has 7.80 GiB memory in use. Process 3826477 has 7.30 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 1.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 594.69 MiB is free. Process 3826476 has 11.89 GiB memory in use. Including non-PyTorch memory, this process has 11.89 GiB memory in use. Process 3826479 has 7.80 GiB memory in use. Process 3826477 has 7.30 GiB memory in use. Of the allocated memory 11.41 GiB is allocated by PyTorch, and 1.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Loading checkpoint shards:   0%|          | 0/15 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 160, in <module>
    main(parse_args())
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 118, in main
    model = LLaMARunner(
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/utils/llama_utils.py", line 29, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 812, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 18.69 MiB is free. Process 3826476 has 11.89 GiB memory in use. Process 3826478 has 11.89 GiB memory in use. Including non-PyTorch memory, this process has 7.83 GiB memory in use. Process 3826477 has 7.83 GiB memory in use. Of the allocated memory 7.35 GiB is allocated by PyTorch, and 1.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Loading checkpoint shards:   0%|          | 0/15 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 160, in <module>
    main(parse_args())
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py", line 118, in main
    model = LLaMARunner(
  File "/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/utils/llama_utils.py", line 29, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3531, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3958, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/transformers/modeling_utils.py", line 812, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 18.69 MiB is free. Process 3826476 has 11.89 GiB memory in use. Process 3826478 has 11.89 GiB memory in use. Process 3826479 has 7.83 GiB memory in use. Including non-PyTorch memory, this process has 7.83 GiB memory in use. Of the allocated memory 7.35 GiB is allocated by PyTorch, and 1.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2025-01-21 10:36:17,045] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3826476) of binary: /home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/bin/python
Traceback (most recent call last):
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/torch/distributed/run.py", line 816, in <module>
    main()
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/hpc/v100dd/v100dd12/anaconda3/envs/openeqa/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/hpc/v100dd/v100dd12/code/open-eqa/openeqa/baselines/llama.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-01-21_10:36:17
  host      : a0702.nhr.fau.de
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3826477)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-01-21_10:36:17
  host      : a0702.nhr.fau.de
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3826478)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-01-21_10:36:17
  host      : a0702.nhr.fau.de
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3826479)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-21_10:36:17
  host      : a0702.nhr.fau.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3826476)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]
=== JOB_STATISTICS ===
=== current date     : Tue Jan 21 10:36:17 CET 2025
= Job-ID             : 2318251 on alex
= Job-Name           : Llama-2-70b-hf
= Job-Command        : /home/hpc/v100dd/v100dd12/code/open-eqa/batch_baseline_a100.sh
= Initial workdir    : /home/hpc/v100dd/v100dd12/code/open-eqa
= Queue/Partition    : a100
= Slurm account      : v100dd with QOS=normal
= Features           : a100_40
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 00:01:52
= Total RAM usage    : 4.8 GiB of assigned  GiB (%)
= Node list          : a0702
= Subm/Elig/Start/End: 2025-01-21T09:20:07 / 2025-01-21T09:20:12 / 2025-01-21T10:34:25 / 2025-01-21T10:36:17
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           99.7G   104.9G   209.7G        N/A     487K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:0E:00.0, 3826476, 4 %, 0 %, 12180 MiB, 14816 ms
NVIDIA A100-SXM4-40GB, 00000000:0E:00.0, 3826479, 3 %, 0 %, 8020 MiB, 9087 ms
NVIDIA A100-SXM4-40GB, 00000000:0E:00.0, 3826477, 8 %, 0 %, 8020 MiB, 9036 ms
NVIDIA A100-SXM4-40GB, 00000000:0E:00.0, 3826478, 1 %, 0 %, 12180 MiB, 15808 ms
NVIDIA A100-SXM4-40GB, 00000000:13:00.0, 3826476, 0 %, 0 %, 492 MiB, 14792 ms
NVIDIA A100-SXM4-40GB, 00000000:13:00.0, 3826479, 0 %, 0 %, 492 MiB, 9079 ms
NVIDIA A100-SXM4-40GB, 00000000:13:00.0, 3826477, 0 %, 0 %, 492 MiB, 9021 ms
NVIDIA A100-SXM4-40GB, 00000000:13:00.0, 3826478, 0 %, 0 %, 492 MiB, 15782 ms
NVIDIA A100-SXM4-40GB, 00000000:49:00.0, 3826476, 0 %, 0 %, 492 MiB, 14763 ms
NVIDIA A100-SXM4-40GB, 00000000:49:00.0, 3826479, 0 %, 0 %, 492 MiB, 9067 ms
NVIDIA A100-SXM4-40GB, 00000000:49:00.0, 3826477, 0 %, 0 %, 492 MiB, 9012 ms
NVIDIA A100-SXM4-40GB, 00000000:49:00.0, 3826478, 0 %, 0 %, 492 MiB, 15757 ms
NVIDIA A100-SXM4-40GB, 00000000:4F:00.0, 3826476, 0 %, 0 %, 492 MiB, 14736 ms
NVIDIA A100-SXM4-40GB, 00000000:4F:00.0, 3826479, 0 %, 0 %, 492 MiB, 9058 ms
NVIDIA A100-SXM4-40GB, 00000000:4F:00.0, 3826477, 0 %, 0 %, 492 MiB, 9004 ms
NVIDIA A100-SXM4-40GB, 00000000:4F:00.0, 3826478, 0 %, 0 %, 492 MiB, 15730 ms
